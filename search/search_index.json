{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the documentation for my infrastructure. Here you will find information, setups and examples about my project.</p> <p>Some parts may be redacted, so expect a few 404 here and there. But if you need more information on anything feel free to contact me at xorob0@posteo.net</p> <p>You will find documentation for my servers:</p> <ul> <li>Celty</li> <li>Nimbus</li> <li>Tinker</li> </ul> <p>And for other things:</p> <ul> <li>Others</li> </ul>"},{"location":"AIDA/","title":"Index","text":"<p>AIDA is my Home Assistant box. It's again a simple Raspberry Pi, this time with 8Gb of RAM and no hard drive. I needed an onsite dedicated box for my home automation because I never want it to be down for any reason. Ever.</p> <p>It's running Ubuntu Server and not Home Assistant OS. This is because I wanted more control over the device (to add exporters, install custom containers,...)</p> <p>It's named after AIDA from Agent of Shield, a robot that read the Darkhold and became an overpowered evil AI trying to become human.</p> <p></p>"},{"location":"AIDA/nebula/","title":"Installing on raspberry pi","text":"<pre><code>mkdir /opt/nebula\ncd /opt/nebula\nwget https://github.com/slackhq/nebula/releases/download/v1.5.0/nebula-linux-arm64.tar.gz\nsudo mkdir /opt/nebula\nsudo tar -C /opt/nebula -xvf nebula-linux-arm64.tar.gz\nrm -Rf /opt/nebula/nebula-linux-arm64.tar.gz\ncd /opt/nebula\n</code></pre>"},{"location":"Macos/Setup/","title":"Setup","text":"<p>This is just some notes I'll be taking while setting up my Macbook from scratch</p>"},{"location":"Macos/Setup/#apps-to-install","title":"Apps to install","text":"<ul> <li>Magnet</li> <li>Karabiner</li> <li>HazeOver</li> <li>iterm2</li> </ul>"},{"location":"Macos/Setup/#others","title":"Others","text":""},{"location":"Macos/Setup/#key-repeat-rate","title":"Key repeat rate","text":"<pre><code>defaults write -g InitialKeyRepeat -int 10\ndefaults write -g KeyRepeat -int 1\ndefaults write -g ApplePressAndHoldEnabled -bool false\n</code></pre> <p>https://amanhimself.dev/blog/setup-macbook-m1/</p>"},{"location":"Macos/nebula/","title":"Nebula","text":""},{"location":"Macos/nebula/#install","title":"Install","text":"<pre><code>brew install nebula\n</code></pre>"},{"location":"Macos/nebula/#config","title":"config","text":"<pre><code>pki:\n  ca: /opt/nebula/ca.crt\n  cert: /opt/nebula/macbook.crt\n  key: /opt/nebula/macbook.key\nstatic_host_map:\n  \"10.200.0.1\": [\"195.201.24.209:4242\"]\nlighthouse:\n  am_lighthouse: false\n  interval: 60\n  hosts:\n    - \"10.200.0.1\"\nlisten:\n  host: 0.0.0.0\n  port: 0\npunchy:\n  punch: true\ntun:\n  disabled: false\n  dev: utun9\n  drop_local_broadcast: false\n  drop_multicast: false\n  tx_queue: 500\n  mtu: 1300\n  routes:\n  unsafe_routes:\nlogging:\n  level: info\n  format: text\nfirewall:\n  conntrack:\n    tcp_timeout: 12m\n    udp_timeout: 3m\n    default_timeout: 10m\n    max_connections: 100000\n  outbound:\n    - port: any\n      proto: any\n      host: any\n  inbound:\n    - port: any\n      proto: any\n      host: any\n</code></pre>"},{"location":"Macos/nebula/#autostart","title":"Autostart","text":"<p>Add <code>/Library/LaunchDaemons/com.nebula.plist</code></p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n  &lt;dict&gt;\n      &lt;key&gt;Label&lt;/key&gt;\n      &lt;string&gt;/usr/local/bin/nebula&lt;/string&gt;\n      &lt;key&gt;LaunchOnlyOnce&lt;/key&gt;\n      &lt;true/&gt;\n      &lt;key&gt;ProgramArguments&lt;/key&gt;\n      &lt;array&gt;\n          &lt;string&gt;/usr/local/bin/nebula&lt;/string&gt;\n          &lt;string&gt;-config&lt;/string&gt;\n          &lt;string&gt;/opt/nebula/config.yml&lt;/string&gt;\n      &lt;/array&gt;\n      &lt;key&gt;RunAtLoad&lt;/key&gt;\n      &lt;true/&gt;\n  &lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre>"},{"location":"celty/","title":"Index","text":"<p>Celty is my media server, she mainly runs my Plex Media Server and some related services.</p>"},{"location":"celty/delete%20unlinked%20torrents/","title":"Delete unlinked torrents","text":"<pre><code>find /HDD1/Media/Downloads -size +50M -links 1 -type f -print\n</code></pre> <p>Will show all files that have no hard link and that are bigger than 50M.  Replace <code>-print</code> by <code>-delete</code> to delete them</p> <p>Now you may want to delete all snapshots of <code>HDD1/Media</code> with this command:</p> <pre><code>zfs list -H -o name -t snapshot HDD1/Media | xargs -n1 zfs destroy\n</code></pre>"},{"location":"celty/host/docker/","title":"Docker","text":""},{"location":"celty/host/docker/#installation","title":"Installation","text":"<pre><code>apt remove docker docker-engine docker.io containerd runc\napt update\napt install apt-transport-https ca-certificates curl gnupg lsb-release\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\napt update\napt install docker-ce docker-ce-cli containerd.io\nsystemctl start docker\nsystemctl enable docker\n</code></pre>"},{"location":"celty/host/docker/#portainer-setup","title":"Portainer Setup","text":"<p>As I do almost everything from portainer I just fire it up and do everything from the command line.</p> <pre><code>docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /configs/portainer:/data portainer/portainer-ce\n</code></pre> <p>After that I generate portainer from portainer </p>"},{"location":"celty/host/hardware/","title":"Hardware","text":""},{"location":"celty/host/hardware/#hardware","title":"Hardware","text":"<p>TODO: Picture of Celty (the server)</p> <p>It's a i3-10100 with 16Gb of DDR4 and a lot of storage. The OS is installed on a ZFS mirrored NVME drive for reliability.</p> <p>One of the issue I faced is my motherboard disabling 2 SATA port when adding a second NVME drive. To fix this I used a passive m.2 to PCIe board from Aliexpress.</p> <p>I have a RAID card, but it's not in use at the moment</p> <ul> <li>CPU: i3-10100</li> <li>Motherboard: MSI Z490-A PRO</li> <li>RAM: 1x16Go</li> <li>Boot drive: 2x Sandisk Extreme Pro 500Gb</li> <li>Cooler: Evo 212</li> <li>Case: Nanoxia Deep Silence 3</li> <li>9750-8i</li> </ul> <p>What I looked after when building this machine was:</p> <ul> <li>A good iGPU for Plex transcoding</li> <li>A lot of SATA port</li> <li>Expandable in the future:</li> <li>Z490 chips that is compatible with 11th gen CPU (and Xe iGPUs)</li> <li>2.5Gbps Ethernet</li> <li>lots of PCIe ports</li> </ul>"},{"location":"celty/host/hardware/#hard-drives","title":"Hard Drives","text":"<p>I currently have 7 hard drives in this box:</p> <ul> <li>2*4To</li> <li>2*6To</li> <li>2*12To</li> <li>1*480Go SSD</li> </ul> <p>I now shuck my hard drives TODO: make a shucking explanation</p>"},{"location":"celty/host/proxmox/","title":"Software","text":"<p>She runs Proxmox, mainly because its one of the few server distro that allow for root on ZFS in the installer. I might use some of the VM capability one day, but for the moment everything runs on Docker straight from the host.</p>"},{"location":"celty/host/proxmox/#proxmox","title":"Proxmox","text":""},{"location":"celty/host/proxmox/#setup","title":"Setup","text":"<ul> <li>Download the latest image</li> <li>Flash on an usb stick</li> <li>Install from GUI</li> </ul>"},{"location":"celty/host/proxmox/#tips","title":"Tips","text":""},{"location":"celty/host/proxmox/#edge-kernel","title":"Edge Kernel","text":"<p>EDIT: this is not needed anymore with PVE 7.1</p> <p>I had to install the edge kernel to get my ethernet card working:</p> <pre><code>cd /tmp\nwget $(curl -s https://api.github.com/repos/fabianishere/pve-edge-kernel/releases/latest | grep 'browser_' | cut -d\\\" -f4 | grep headers)\nwget $(curl -s https://api.github.com/repos/fabianishere/pve-edge-kernel/releases/latest | grep 'browser_' | cut -d\\\" -f4 | grep kernel | head -1)\napt install pve-edge-*.deb\n</code></pre>"},{"location":"celty/host/proxmox/#pve-test-repository","title":"PVE Test Repository","text":"<p>To have a recent enough version of ZFS on Linux I needed to install the PVE Test Repository.</p> <pre><code>echo \"deb http://download.proxmox.com/debian/pve buster pvetest\" &gt;&gt; /etc/apt/sources.list\napt update\napt upgrade\n</code></pre>"},{"location":"celty/host/storage/","title":"Storage","text":""},{"location":"celty/host/storage/#configure-3ware-lsi-card-on-proxmox-64","title":"Configure 3Ware LSI card on Proxmox 6.4","text":"<p>This was only tested on 9750-4i and 9750-8i</p>"},{"location":"celty/host/storage/#install-the-repositories","title":"Install the repositories","text":"<pre><code>cd /tmp\nwget http://hwraid.le-vert.net/debian/hwraid.le-vert.net.gpg.key\napt-key add hwraid.le-vert.net.gpg.key\nrm hwraid.le-vert.net.gpg.key\n</code></pre>"},{"location":"celty/host/storage/#install-tw-cli","title":"Install tw-cli","text":"<pre><code>apt update\napt install tw-cli\n</code></pre>"},{"location":"celty/host/storage/#setup-disk","title":"Setup disk","text":""},{"location":"celty/host/story/","title":"Story","text":""},{"location":"celty/host/story/#story","title":"Story","text":""},{"location":"celty/host/story/#hardware","title":"Hardware","text":"<p>Let's start with the story of how I started sefhosting. This was on a Raspberry Pi 2B with a 2Tb usb drive, the only thing it did back then was download files that I could access via SSHFS. This was really terrible, this setup could only withstand a few downloads at a time and I had to pause all downloads to access the files.</p> <p>Then came Celty. I got her for 50\u20ac from a acquaintance. She was only a 4th gen i5 with 8Gb of RAM and a few hard drives. She was going to stay in my bedroom so I had to make her as silent as possible. This is why I got an Evo 212 and a Nanoxia Deep Silence 3. At that point I installed FreeNAS on her and started playing with jails, I selfhosted a bunch of stuff, and started de-googlizing my life.</p> <p>When I moved to my first apartment is was in my TV closet, so the silent factor was still really important.</p>"},{"location":"celty/host/story/#software","title":"Software","text":"<p>The Software side of things had a lot more iterations than the hardware side.</p> <p>Back on my Raspberry Pi I was on Open Media Vault and really didn't like it. Every update broke the system, but I didn't have a lot of experience back then so that may be my fault too \ud83e\udd37.</p> <p>On Celty I used FreeNAS for a long time. Back in the days it was basically the only way to have ZFS with good performances. I kept FreeNAS for a few years until the release of OpenZFS 2.0.0 which made it possible to move to Linux.</p> <p>At that time I a friend was setting up his server with OMV and docker and the simply of docker really intrigued me. That's why on December 2020 I moved to Ubuntu 20.04 with docker and OpenZFS. I loved it even if the OpenZFS setup was quite messy because it was not yet officially supported by Ubuntu.</p> <p>A few weeks later I upgraded my hardware and kind of broke my install. I decided to go with Proxmox VE as it was supposed to have a really up to date OpenZFS version. I discovered this was false and had to install the beta kernel to have it working.</p> <p>On my latest setup I tried TrueNAS SCALE which just lacks too much docker features for me at the moment and Ubuntu server which I dumped because it doesn't support root on ZFS right now.</p>"},{"location":"celty/host/story/#name","title":"Name","text":"<p>It's named after Celty Sturluson, a character from Durarara!! who is black, mute, rides a noise-less bike and is literally headless. </p>"},{"location":"celty/host/zfs/","title":"ZFS","text":"<p>With my latest disk upgrade I started managing 2 pools, a stripped pool with most of my disks (badly named <code>HDD1</code>) and an other stripped pool with the disks I don't want in my main pool (named <code>extra</code>).</p> <p>As explained above, I don't want my main pool to ever get too big, this reduces the risk that one of my disk would fail and also let me keep one SATA port available at all time in case I need to replace a drive.</p> <p>I only have a few datasets:</p> <ul> <li><code>HDD1/Media</code>: with all my medias (and my download folder). This needs to be on the same dataset so I can hardlink my movies from my download folder to my Movies folder (and avoid duplicates)</li> <li><code>HDD1/Documents</code>: with my NextCloud storage, including my pictures</li> <li><code>HDD1/Backups</code>: with some old unorganized backups and some new Time Machine backups</li> <li><code>HDD1/Public</code>: with data from linx, it's limited to 200Go</li> </ul> <p>I also have a ZFS root (<code>rpool</code>) partition with a few datasets</p> <ul> <li><code>rpool/configs</code>: all my configs folders that I mount on my docker containers</li> <li><code>rpool/configs/plex</code>: some of my container have their own datasets so it's easier to backup</li> </ul> <p><code>extra</code> is only used as a backup pool locally, all the dataset there are created from ZFS send/recv.</p>"},{"location":"celty/host/zfs/#migration","title":"Migration","text":"<p>To migrate from an existing Proxmox VE to a new install, all I have to do is:</p> <pre><code>rsync -av /mnt/old/configs /configs\nrsync -av /mnt/old/var/lib/docker /docker\n</code></pre>"},{"location":"celty/services/bazarr/","title":"Bazarr","text":"<p>Bazarr is a companion service to Sonarr and Radarr that autmatically fetches subtitles from various sources</p>"},{"location":"celty/services/bazarr/#compose","title":"Compose","text":"<pre><code>version: \"3.3\"\nservices:\n  bazarr:\n    image: ghcr.io/linuxserver/bazarr\n    container_name: bazarr\n    volumes:\n      - /configs/bazarr:/config\n      - /HDD1/Media:/media\n    restart: unless-stopped\n</code></pre>"},{"location":"celty/services/caddy/","title":"Caddy","text":"<p>Caddy is my reverse proxy of choice. It manages Let's Encrypt on it's own and only needs a config file.</p>"},{"location":"celty/services/caddy/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  caddy:\n    image: caddy\n    container_name: caddy\n    restart: unless-stopped\n    volumes:\n      - /configs/caddy/data/:/data\n      - /configs/caddy/Caddyfile:/etc/caddy/Caddyfile\n    ports:\n      - 443:443\n      - 80:80\n      - 8448:8448\n</code></pre> <p>I just need to expose every port I want to use in in the <code>ports</code> section.</p>"},{"location":"celty/services/caddy/#config","title":"Config","text":"<pre><code>{\n  http_port 80\n  https_port 443\n  email letsencrypt@toum.me\n}\n\n(nobots) {\n  @bot {\n    header_regexp User-Agent Googlebot|aolbuild|baidu|bingbod|bingpreview|msnbot|duckduckbot|googlebot|adbot-google|mediapartners-google|teoma|slurp|yandex|Indy*Library|libwww-perl|GetRight|GetWeb!|Go!Zilla|Download*Demon|Go-Ahead-Got-It|TurnitinBot|GrabNet\n  }\n\n  @badwords {\n    path_regexp \\b(ultram|unicauca|valium|viagra|vicodin|xanax|ypxaieo|erections|hoodia|huronriveracres|impotence|levitra|libido|ambien|blue\\spill|cialis|cocaine|ejaculation|erectile|lipitor|phentermin|pro[sz]ac|sandyauer|tramadol|troyhamby)\\b\n  }\n\n  @sql {\n    path_regexp \\b(union.*select.*\\(|union.*all.*select.*|concat.*\\()\\b\n  }\n\n  @block {\n    path_regexp \\b([a-zA-Z0-9_]=http://|[a-zA-Z0-9_]=(\\.\\.//?)+|[a-zA-Z0-9_]=/([a-z0-9_.]//?)+)\\b\n  }\n\n  @exploits {\n    path_regexp \\b((&lt;|%3C).*script.*(&gt;|%3E)|GLOBALS(=|\\[|\\%[0-9A-Z]{0,2})|_REQUEST(=|\\[|\\%[0-9A-Z]{0,2})|proc/self/environ|mosConfig_[a-zA-Z_]{1,21}(=|\\%3D)|base64_(en|de)code\\(.*\\))\\b\n }\n\n  respond @bot 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @badwords 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @sql 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @block 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @exploits 403 {\n    body \"Not dank enough\"\n    close\n  }\n\n  header Permissions-Policy \"interest-cohort=()\"\n\n  respond /robots.txt 200 {\n    body \"User-agent: *\n    Disallow: /\"\n  }\n}\n\n(management) {\n  @external {\n     not remote_ip 10.200.0.0/24\n  }\n  respond @external 503 {\n    body \"Server is having issues\"\n  }\n  tls internal\n}\n\ncaddy.celty {\n  metrics /metrics\n  import management\n}\ncadvisor.celty {\n  reverse_proxy {\n    to cadvisor:8080\n  }\n  import management\n}\nnodeexporter.celty {\n  reverse_proxy {\n    to nodeexporter:9100\n  }\n  import management\n}\nnodeexporter.bifrost {\n  reverse_proxy {\n    to 192.168.2.1:9100\n  }\n  import management\n}\ncadvisor.aida {\n  reverse_proxy {\n    to 192.168.2.65:8080\n  }\n  import management\n}\nnodeexporter.aida {\n  reverse_proxy {\n    to 192.168.2.65:9100\n  }\n  import management\n}\ntorrents.celty {\n  reverse_proxy {\n    to flood:3000\n  }\n  import management\n}\nugly.torrents.celty {\n  reverse_proxy {\n    to qbittorrent:8080\n  }\n  import management\n}\ns3.gneee.tech {\n  reverse_proxy {\n    to minio:9000\n  }\n  import nobots\n}\nradarr.celty {\n  reverse_proxy {\n    to radarr:7878\n  }\n  import management\n}\nsonarr.celty {\n  reverse_proxy {\n    to sonarr:8989\n  }\n  import management\n}\nlidarr.celty {\n  reverse_proxy {\n    to lidarr:8686\n  }\n  import management\n}\nprint.celty {\n  reverse_proxy {\n    to 192.168.2.37:80\n  }\n  import management\n}\nlinx.gneee.tech {\n  reverse_proxy {\n    to linx-server:8080\n  }\n  import nobots\n}\nplex.gneee.tech {\n  reverse_proxy {\n    to plex:32400\n  }\n  import nobots\n}\nbazarr.celty {\n  reverse_proxy {\n    to bazarr:6767\n  }\n  import management\n}\ntautulli.celty {\n  reverse_proxy {\n    to tautulli:8181\n  }\n  import management\n}\nebooks.celty {\n  reverse_proxy {\n    to calibre-web:8083\n  }\n  import management\n}\ncalibre.celty {\n  reverse_proxy {\n    to calibre:8080\n  }\n  import management\n}\ndupeguru.celty {\n  reverse_proxy {\n    to dupeguru:5800\n  }\n  import management\n}\nphotos.celty {\n  reverse_proxy {\n    to librephotos:80\n  }\n  import management\n}\ncloud.celty {\n  reverse_proxy {\n    to nextcloud:80\n  }\n  import management\n}\n</code></pre> <p>As much as possible I try to use docker's hostnames, this make this setup really portable and easy to maintain.</p>"},{"location":"celty/services/caddy/#cool-snippets","title":"Cool snippets","text":""},{"location":"celty/services/caddy/#no-bots","title":"No bots","text":"<p>I wrote a few rules that can help decrease the risks of being indexed or scanned by script kiddies on every subdomains. They are inspired from the nginx no bot config.</p> <pre><code>(nobots) {\n  @bot {\n    header_regexp User-Agent Googlebot|aolbuild|baidu|bingbod|bingpreview|msnbot|duckduckbot|googlebot|adbot-google|mediapartners-google|teoma|slurp|yandex|Indy*Library|libwww-perl|GetRight|GetWeb!|Go!Zilla|Download*Demon|Go-Ahead-Got-It|TurnitinBot|GrabNet\n  }\n\n  @badwords {\n    path_regexp \\b(ultram|unicauca|valium|viagra|vicodin|xanax|ypxaieo|erections|hoodia|huronriveracres|impotence|levitra|libido|ambien|blue\\spill|cialis|cocaine|ejaculation|erectile|lipitor|phentermin|pro[sz]ac|sandyauer|tramadol|troyhamby)\\b\n  }\n\n  @sql {\n    path_regexp \\b(union.*select.*\\(|union.*all.*select.*|concat.*\\()\\b\n  }\n\n  @block {\n    path_regexp \\b([a-zA-Z0-9_]=http://|[a-zA-Z0-9_]=(\\.\\.//?)+|[a-zA-Z0-9_]=/([a-z0-9_.]//?)+)\\b\n  }\n\n  @exploits {\n    path_regexp \\b((&lt;|%3C).*script.*(&gt;|%3E)|GLOBALS(=|\\[|\\%[0-9A-Z]{0,2})|_REQUEST(=|\\[|\\%[0-9A-Z]{0,2})|proc/self/environ|mosConfig_[a-zA-Z_]{1,21}(=|\\%3D)|base64_(en|de)code\\(.*\\))\\b\n }\n\n  respond @bot 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @badwords 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @sql 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @block 403 {\n    body \"Not dank enough\"\n    close\n  }\n  respond @exploits 403 {\n    body \"Not dank enough\"\n    close\n  }\n\n  header Permissions-Policy \"interest-cohort=()\"\n\n  respond /robots.txt 200 {\n    body \"User-agent: *\n    Disallow: /\"\n  }\n}\n\n\n</code></pre> <p>These rules respond with error for commonly used bad word, exploit and for all bots (based on their user agent). It also automatically add a <code>/robots.txt</code> that tells indexers not to index any page of this website.</p>"},{"location":"celty/services/caddy/#management","title":"Management","text":"<p>For services that do not need to be accessed by other people I restrict access to only the IPs from my nebula setup. I also use a self signed tls to be able to use <code>.celty</code> domains.</p> <pre><code>(management) {\n  @external {\n     not remote_ip 10.200.0.0/24\n  }\n  respond @external 403 {\n    body \"Not dank enough\"\n  }\n  tls internal\n}\n</code></pre>"},{"location":"celty/services/calibre/","title":"Calibre","text":"<p>TODO</p> <pre><code>version: '3.3'\nservices:\n  calibre:\n    image: ghcr.io/linuxserver/calibre\n    container_name: calibre\n    environment:\n      - PASSWORD=password\n    volumes:\n      - /configs/calibre:/config\n      - /HDD1/Media/Books:/books\n    restart: unless-stopped\n  calibre-web:\n    image: ghcr.io/linuxserver/calibre-web\n    container_name: calibre-web\n    environment:\n      - DOCKER_MODS=linuxserver/calibre-web:calibre\n    volumes:\n      - /configs/calibre/web:/config\n      - /configs/calibre:/calibre\n      - /HDD1/Media/Books:/books\n    ports:\n      - 8083:8083\n    restart: unless-stopped\nnetworks:\n  default:\n    external:\n      name: external\n</code></pre>"},{"location":"celty/services/linx/","title":"Linx","text":"<p>Linx is a file host with an api that's compatible with <code>wget</code></p>"},{"location":"celty/services/linx/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  linx-server:\n    container_name: linx-server\n    image: andreimarcu/linx-server\n    command: -config /data/linx-server.conf\n    volumes:\n      - /HDD1/:/HDD1/Public\n      - /configs/linx/meta:/data/meta\n      - /configs/linx/linx-server.conf:/data/linx-server.conf\n    restart: unless-stopped\n</code></pre>"},{"location":"celty/services/linx/#config","title":"Config","text":"<pre><code>siteurl = https://linx.gneee.tech/\nsitename = Linx\nmaxsize = 4294967296\nmaxexpiry = 2592000\n</code></pre>"},{"location":"celty/services/minio/","title":"Minio","text":"<p>TODO</p>"},{"location":"celty/services/plex/","title":"Plex","text":"<p>Plex Media Server is a streaming service you can host at home (with your own content).</p>"},{"location":"celty/services/plex/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  plex:\n    image: ghcr.io/linuxserver/plex\n    container_name: plex\n    restart: unless-stopped\n    volumes:\n      - /configs/plex:/config\n      - /HDD1/Media/TV-Shows:/tv\n      - /HDD1/Media/Movies:/movies\n      - /HDD1/Media/Lives:/lives\n    tmpfs:\n      - /tmp\n    devices:\n      - /dev/dri:/dev/dri\n</code></pre> <p>I'm using linuxserver.io's image</p>"},{"location":"celty/services/plex/#tmpfs","title":"tmpfs","text":"<p>Nothing really special on this docker-compose, except for the <code>tmpfs</code> line.</p> <p>You can thing of <code>tmpfs</code> as a \"mountpoint to RAM\". Every files written to <code>/tmp</code> is written to RAM.</p> <p>I am using this so my transcoded files don't have to be written to my SSD (because it would worn it out pretty fast) or my ZFS pool (which is slow in comparaison to RAM or my SSD).</p>"},{"location":"celty/services/plex/#devices","title":"devices","text":"<p>To use QuickSync for transcoding I need to pass <code>/dev/dri</code> to the container. After that everything just works.</p>"},{"location":"celty/services/plex/#tips","title":"Tips","text":""},{"location":"celty/services/plex/#put-your-thumnails-on-your-ssd","title":"Put your thumnails on your SSD","text":"<p>I used to have my thumbnails and my preview images on my HDD, this makes the UI feels really slow. But on the other hand these images can take up a lot of space, I'm currently at 50Go for my Plex Media Server folder. This could be mitigated with a CDN but might get you bad performances on local network.</p>"},{"location":"celty/services/plex/#get-plex-pass","title":"Get Plex Pass","text":"<p>If you want to use Plex the way I am, get a Plex Pass! This will unlock some really nice features and also support the devs</p>"},{"location":"celty/services/plex/#disable-online-media-sources","title":"Disable Online Media Sources","text":"<p>One of the first things I did was disabling the free plex sources, I don't use them and find that they mostly clutter the UI.  The only one I kept enabled is the Tidal one. I advise you to keep it enabled and add a free tidal account. This will add the soudtracks to your movies and TV shows.</p>"},{"location":"celty/services/plex/#remote-access","title":"Remote access","text":"<p>Plex comes with integrated remote access management. It works well, but does not allow for a lot of optimisation. I decided to not use it.</p> <p></p> <p>Instead I set up a \"Custom server access URL\" under \"Network\"</p> <p></p> <p>For this to work I am routing with Caddy</p> <p>This has several benefits:</p> <ul> <li>I don't get blocked by firewall when trying to listen to my music at the ofice</li> <li>I can use cloudflare's CDN for my images (they don't allow videos to be cached anymore)</li> <li>Every services available outside my network is managed by my reverse proxy, meaning I have more control about it.</li> </ul> <p>You need to specify the port even if using some standard http/https ports.</p>"},{"location":"celty/services/plex/#dont-use-a-cdn-if-youre-going-to-use-plex-locally","title":"Don't use a CDN if you're going to use Plex locally","text":"<p>If you use a CDN evey stream will have to go through the CDN, this will give you pretty bad perfomances on local network even if it can improves performances remotely.</p> <p>This could be circumvented with some DNS magic but I did not try it yet.</p>"},{"location":"celty/services/plex/#transcoder-settings","title":"Transcoder settings","text":"<p>If you want to use <code>tmpfs</code> as your transcoding folder, you should set it in the \"Transcoder\" settings:</p> <p></p> <p>I advise you to use <code>/tmp</code> since I had some issues using other directories. It seems like the library used for EAC decoding was not using the path set up in the settings and would always go to <code>/tmp</code>.</p> <p>If it is not enabled, you should enable:</p> <ul> <li>HDR tonemapping: This makes HDR content compatible with SDR devices. If disabled your HDR content will look washed out on SDR devices</li> <li>Use hardware acceleration when available: This will use your GPU instead of you CPU for transocding</li> </ul>"},{"location":"celty/services/plex/#enable-agents","title":"Enable agents","text":"<p>In the \"Agents\" settings you can add some agents that require API keys, they will make more content available like alternatives posters for your movies. They might also help with content matching.</p>"},{"location":"celty/services/radarr/","title":"Radarr","text":"<p>Radarr is a movie manager that can grab movies using torrents.</p>"},{"location":"celty/services/radarr/#compose","title":"Compose","text":"<pre><code>  radarr:\n    image: ghcr.io/linuxserver/radarr\n    container_name: radarr\n    restart: unless-stopped\n    volumes:\n      - /configs/radarr:/config\n      - /HDD1/Media:/media\n</code></pre> <p>I'm using linuxserver.io's image</p>"},{"location":"celty/services/radarr/#tips","title":"Tips","text":""},{"location":"celty/services/radarr/#import-existing-library","title":"Import existing library","text":"<p>I you already have a library but want to tidy it up you can follow these steps:</p> <ul> <li>Library Import</li> <li>Add you library</li> <li>Match you movies</li> <li>Go to the movie list and click \"edit movies\"</li> <li>Select everything</li> <li>Change root folder (even if they already are in this folder)</li> <li>Rename</li> </ul>"},{"location":"celty/services/radarr/#settings","title":"Settings","text":""},{"location":"celty/services/radarr/#media-management","title":"Media management","text":"<p>Enable \"Import Extra Files\" and set the value to <code>srt,nfo,sub</code> to import subtitle and <code>nfo</code> files</p>"},{"location":"celty/services/radarr/#profiles","title":"Profiles","text":"<p>I like to create a <code>Best</code> profile that will only take movies in quality &gt;= 720p and keep upgrading until it's a 4k remux</p> <p>1080p Remuxes are better than 2160p Web-DL (especially with the Shield's AI upscaling) so don't forget to change the order.</p>"},{"location":"celty/services/radarr/#custom-format","title":"Custom Format","text":"<p>I highly suggest you take a look at TRaSH's Guide's Custom Formats and add the one you want.</p> <p></p> <p>And don't forget to add them to your profiles</p> <p></p>"},{"location":"celty/services/radarr/#indexers","title":"Indexers","text":"<p>Don't bother with Radarr's built in indexers support, use jackett for everything. It has better search and rss support and get updated more often.</p> <p>You can add some restrictions to your releases. On my setup I restrict anything containing <code>XXX</code> or <code>Porn</code> to be sure to never get adult parody of my movies. This could cause some issue with some movies containing those terms through.</p>"},{"location":"celty/services/radarr/#connect","title":"Connect","text":"<p>It might look dumb to enable connect at first since Plex automatically scan new files. But if you've got a upgrade Plex will only scan the movie if the name changes, it's always better to have it enabled.</p>"},{"location":"celty/services/radarr/#general","title":"General","text":"<p>Don't forget to enable security and disable analytics.</p>"},{"location":"celty/services/sonarr/","title":"Sonarr","text":"<p>Sonarr is a TV Shows manager that can grab episodes using torrents.</p>"},{"location":"celty/services/sonarr/#compose","title":"Compose","text":"<pre><code>  sonarr:\n    image: ghcr.io/linuxserver/sonarr\n    container_name: sonarr\n    restart: unless-stopped\n    volumes:\n      - /configs/sonarr:/config\n      - /HDD1/Media:/media\n</code></pre> <p>I'm using linuxserver.io's image</p>"},{"location":"celty/services/sonarr/#tips","title":"Tips","text":""},{"location":"celty/services/sonarr/#settings","title":"Settings","text":""},{"location":"celty/services/sonarr/#profiles","title":"Profiles","text":"<p>I like to create a <code>Best</code> profile that will only take movies in quality &gt;= 720p and keep upgrading until it's a 4k remux</p> <p>1080p Remuxes are better than 2160p Web-DL (especially with the Shield's AI upscaling) so don't forget to change the order.</p>"},{"location":"celty/services/sonarr/#indexers","title":"Indexers","text":"<p>Don't bother with Sonarr's built in indexers support, use jackett for everything. It has better search and rss support and get updated more often.</p>"},{"location":"celty/services/sonarr/#connect","title":"Connect","text":"<p>It might look dumb to enable connect at first since Plex automatically scan new files. But if you've got a upgrade Plex will only scan your episodes if the name changes, it's always better to have it enabled.</p>"},{"location":"celty/services/sonarr/#general","title":"General","text":"<p>Don't forget to enable security and disable analytics.</p>"},{"location":"celty/services/tautulli/","title":"Tautulli","text":"<p>Tautulli is a tool monitor you Plex server, it can give you stats, a dashboard and notifications.</p>"},{"location":"celty/services/tautulli/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  tautulli:\n    container_name: tautulli\n    image: tautulli/tautulli\n    restart: unless-stopped\n    volumes:\n      - /configs/tautulli:/config\n      - /configs/plex/Library/Application Support/Plex Media Server/Logs/:/plex_logs:ro\n</code></pre>"},{"location":"celty/services/tautulli/#tips","title":"Tips","text":""},{"location":"celty/services/tautulli/#downtime-notification","title":"Downtime notification","text":"<p>Tautulli can monitor you Plex Media Server state and alert you if it goes down, on playback error and more. It's a good idea to enable this if you share your server.</p>"},{"location":"celty/services/torrents/","title":"Torrents","text":"<p>TODO</p>"},{"location":"celty/services/torrents/#qbittorrent","title":"QbitTorrent","text":"<p>QbitTorrent is a simple torrent client that has an \"OK\" web UI.</p>"},{"location":"celty/services/torrents/#compose","title":"Compose","text":"<pre><code>version: \"3.3\"\nservices:\n  qbittorrent:\n    image: ghcr.io/linuxserver/qbittorrent\n    container_name: qbittorrent\n    volumes:\n      - /configs/qbitorrent:/config\n      - /HDD1/Media/:/media\n    ports:\n      - 6881:6881\n      - 6881:6881/udp\n    restart: unless-stopped\n</code></pre>"},{"location":"celty/services/torrents/#flood","title":"Flood","text":"<p>Flood is an alternative frontend for torrent clients (in this case QbitTorrent)</p>"},{"location":"celty/services/torrents/#compose_1","title":"Compose","text":"<pre><code>version: \"3.3\"\nservices:\n  flood:\n    container_name: flood\n    image: jesec/flood\n    restart: unless-stopped\n    environment:\n      HOME: /config\n    volumes:\n      - /configs/flood:/config\n      - /configs/flood/data:/data\n      - /HDD1/Media:/media:ro\n</code></pre>"},{"location":"monitoring/blackhole/","title":"Blackhole Exporter","text":"<p>TODO</p>"},{"location":"monitoring/cadvisor/","title":"Cadvisor","text":"<p>TODO</p> <pre><code>version: '3.3'\nservices:\n  cadvisor:\n    image: gcr.io/google-containers/cadvisor\n    container_name: cadvisor\n    volumes:\n      - /:/rootfs:ro\n      - /var/run:/var/run:rw\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    restart: always\n    command: [\"--docker_only=true\"]\n\nnetworks:\n  default:\n    external:\n      name: external\n</code></pre>"},{"location":"monitoring/node/","title":"Node Exporter","text":"<p>TODO</p> <pre><code>version: '3.3'\nservices:\n  nodeexporter:\n    container_name: nodeexporter\n    image: prom/node-exporter\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /var/run:/var/run:rw\n      - /var/lib/docker/:/var/lib/docker:ro\n      - /:/rootfs:ro,rslave\n      - /HDD1/:/media/data:ro\n    command:\n      - '--path.procfs=/host/proc'\n      - '--path.rootfs=/rootfs'\n      - '--path.sysfs=/host/sys'\n      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'\n      - '--collector.textfile.directory=/var/lib/node_exporter/textfile_collector/'\n    restart: always\n\nnetworks:\n  default:\n    external:\n      name: external\n</code></pre>"},{"location":"nimbus/","title":"Index","text":"<p>Nimbus is a VPS, hosted on Hetzner. It's the cheapest VPS they offer with a single CPU core, 2Gb of RAM and 20Gb of SSD. I added two 10Gb volumes for my ZFS pool.</p> <p>I went with a VPS over a true selfhosted solution for the better availability. More and more people around me depend on my services so I need some reliability.</p> <p></p> <p>It runs ubuntu server and docker for the few services on there.</p> <p>It's named Nimbus because it's in the cloud. </p>"},{"location":"nimbus/host/VPS/","title":"Hetzner VPS","text":"<p>TODO</p>"},{"location":"nimbus/host/debian/","title":"Debian","text":"<p>TODO</p>"},{"location":"nimbus/host/docker/","title":"Docker","text":"<p>TODO</p>"},{"location":"nimbus/services/jackett/","title":"Jackett","text":"<p>Jackett is a proxy that translate arr's request into tracker-specific queries. I use it to connect some trackers to my arr's.</p>"},{"location":"nimbus/services/jackett/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  jackett:\n    image: ghcr.io/linuxserver/jackett\n    container_name: jackett\n    restart: unless-stopped\n    volumes:\n      - /configs/jackett:/config\n</code></pre> <p>I'm using linuxserver.io's image</p>"},{"location":"nimbus/services/jackett/#tips","title":"Tips","text":""},{"location":"nimbus/services/jackett/#password","title":"Password","text":"<p>Don't forget to set a password if you're instance is available outside your LAN</p>"},{"location":"nimbus/services/jackett/#dont-add-too-much-public-trackers","title":"Don't add too much public trackers","text":"<p>When I started all this I used to add as much public trackers as I could. Not only this is a pain in the ass to add to my *arrs, but it also become really slow because it tries to query every websites before adding a movie. Just add the one you really use.</p>"},{"location":"nimbus/services/miniflux/","title":"Miniflux","text":"<p>Miniflux is simply the best rss reader that can be selfhosted. I use it in combinasion with Reeder on my Mac and my iPhone and it's just perfect !</p>"},{"location":"nimbus/services/miniflux/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  miniflux:\n    image: miniflux/miniflux:latest\n    container_name: miniflux\n    depends_on:\n      web_miniflux-db:\n        condition: service_healthy\n    environment:\n      - DATABASE_URL=postgres://miniflux:password@miniflux-db/miniflux?sslmode=disable\n  miniflux-db:\n    image: postgres:latest\n    container_name: miniflux-db\n    environment:\n      - POSTGRES_USER=miniflux\n      - POSTGRES_PASSWORD=password\n    volumes:\n      - web_miniflux-db:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD\", \"pg_isready\", \"-U\", \"miniflux\"]\n      interval: 10s\n      start_period: 30s\nvolumes:\n  miniflux-db:\n</code></pre> <p>I don't usualy setup healthchecks but since it was in the official documentation I let it there</p>"},{"location":"nimbus/services/miniflux/#setup","title":"Setup","text":""},{"location":"nimbus/services/miniflux/#database-initmigration","title":"Database init/migration","text":"<p>This command needs to be ran before the service can start. It will initialize the database if it never was and migrate it if the datase was created on an older/newer version.</p> <pre><code>docker-compose exec miniflux /usr/bin/miniflux -migrate\n</code></pre>"},{"location":"nimbus/services/miniflux/#create-admin","title":"Create admin","text":"<p>No account exist by default so we need to create one.</p> <pre><code>docker-compose exec miniflux /usr/bin/miniflux -create-admin\n</code></pre>"},{"location":"nimbus/services/miniflux/#config","title":"Config","text":""},{"location":"nimbus/services/miniflux/#fever-api","title":"Fever API","text":"<p>If you want to enable the Fever API (for exemple to comunicate with Reeder), login on the web UI and go to Settings &gt; Integrations</p>"},{"location":"nimbus/services/ombi/","title":"Ombi","text":""},{"location":"nimbus/services/ombi/#compose","title":"Compose","text":"<pre><code>version: '3.3'\nservices:\n  ombi:\n    image: ghcr.io/linuxserver/ombi:development\n    container_name: ombi\n    restart: unless-stopped\n    volumes:\n      - /configs/ombi-dev:/config\n</code></pre>"},{"location":"nimbus/services/ombi/#tips","title":"Tips","text":""},{"location":"nimbus/services/ombi/#v3","title":"V3","text":"<p>I really adivise you to use the <code>development</code> tag to enjoy the new version.</p>"},{"location":"nimbus/services/ombi/#css","title":"CSS","text":"<p>To disable the useless tabs I used:</p> <pre><code>#nav-adminDonate {\ndisplay: none\n}\n#nav-featureSuggestion {\ndisplay: none\n}\n</code></pre>"},{"location":"nimbus/services/organizr/","title":"Organizr","text":"<p>Organizr can agregate all my selfhosted web ui into one and generate a cool dashboard with widget from some of them.</p>"},{"location":"nimbus/services/organizr/#compose","title":"Compose","text":"<pre><code>version: \"3.3\"\nservices:\n  organizr:\n    image: organizr/organizr\n    container_name: organizr\n    volumes:\n      - /configs/organizr:/config\n    restart: unless-stopped\n</code></pre>"},{"location":"nimbus/services/portainer/","title":"Portainer","text":""},{"location":"nimbus/services/portainer/#portainer-setup","title":"Portainer Setup","text":"<p>As I do almost everything from portainer I just fire it up and do everything from the commandline.</p> <pre><code>docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v /configs/portainer:/data portainer/portainer-ce\n</code></pre> <p>If I ran the migration lines this part should not even be needed.</p> <p>https://raw.githubusercontent.com/portainer/templates/master/templates-2.0.json</p>"},{"location":"nimbus/services/synapse/","title":"Synapse","text":"<p>TODO</p>"},{"location":"others/","title":"Index","text":""},{"location":"others/nebula/","title":"Nebula","text":""},{"location":"others/nebula/#install-on-ubuntu","title":"Install on ubuntu","text":"<pre><code>mkdir /opt/nebula\ncd /opt/nebula\nwget https://github.com/slackhq/nebula/releases/download/v1.2.0/nebula-linux-amd64.tar.gz\nsudo mkdir /opt/nebula\nsudo tar -C /opt/nebula -xvf nebula-linux-amd64.tar.gz\nsudo ufw allow 4242/udp\ncd /opt/nebula\nchmod +x nebula-cert\n./nebula-cert ca -name \"Managment network\"\nsudo wget https://raw.githubusercontent.com/slackhq/nebula/master/examples/config.yml\n</code></pre> <p>Source: https://blog.galt.me/nebula-mesh-vpn-on-ubuntu/</p>"},{"location":"others/nebula/#create-a-certificate","title":"Create a certificate","text":"<pre><code>./nebula-cert sign -name \"nimbus\" -ip \"10.200.0.1/24\"\n\n./nebula-cert sign -name \"macbook\" -ip \"10.200.0.2/24\" -groups \"laptop,home,ssh\"\n./nebula-cert sign -name \"iphone\" -ip \"10.200.0.3/24\" -groups \"mobile,home,ssh\"\n./nebula-cert sign -name \"celty\" -ip \"10.200.0.4/24\" -groups \"server,home,ssh\"\n./nebula-cert sign -name \"aida\" -ip \"10.200.0.5/24\" -groups \"server,home,ssh\"\n./nebula-cert sign -name \"rocket\" -ip \"10.200.0.6/24\" -groups \"server,home,ssh\"\n./nebula-cert sign -name \"s22\" -ip \"10.200.0.8/24\" -in-pub s22.pub -groups \"server,home,ssh\"\n</code></pre> <p>edit <code>config.yml</code></p>"},{"location":"others/nebula/#setups","title":"Setups","text":""},{"location":"others/nebula/#linux","title":"Linux","text":"<p>Install with:</p> <pre><code>mkdir /opt/nebula\ncd /opt/nebula\nwget https://github.com/slackhq/nebula/releases/download/v1.2.0/nebula-linux-amd64.tar.gz\nsudo mkdir /opt/nebula\nsudo tar -C /opt/nebula -xvf nebula-linux-amd64.tar.gz\n</code></pre> <p>nebula.sh</p> <pre><code>#!/bin/bash\n\n/opt/nebula/nebula -config /opt/nebula/config.yml\n</code></pre> <p>/etc/systemd/system/nebula-start.service</p> <pre><code>[Unit]  \nDescription=Start Nebula Mesh VPN as a service.  \n[Service]  \nType=simple  \nExecStart=/bin/bash /root/nebula.sh  \n[Install]  \nWantedBy=multi-user.target\n</code></pre> <pre><code>sudo chmod 644 /etc/systemd/system/nebula-start.service\nsudo systemctl daemon-reload\nsudo systemctl enable nebula-start.service\nsudo systemctl start nebula-start.service\nsudo systemctl status nebula-start.service\n</code></pre>"},{"location":"others/nebula/#macos","title":"MacOS","text":"<p>Put your <code>crt</code> and <code>key</code> files in <code>/opt/nebula</code></p> <p>/Library/LaunchAgents/nebula.startup.plist</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;\n&lt;plist version=\"1.0\"&gt;\n&lt;dict&gt;\n    &lt;key&gt;Label&lt;/key&gt;\n    &lt;string&gt;com.nebula.startup&lt;/string&gt;\n\n    &lt;key&gt;OnDemand&lt;/key&gt;\n    &lt;false/&gt;\n\n    &lt;key&gt;LaunchOnlyOnce&lt;/key&gt;\n    &lt;true/&gt;\n\n    &lt;key&gt;UserName&lt;/key&gt;\n    &lt;string&gt;xorob0&lt;/string&gt;\n\n    &lt;key&gt;ProgramArguments&lt;/key&gt;\n    &lt;array&gt;\n        &lt;string&gt;/usr/local/bin/nebula&lt;/string&gt;\n        &lt;string&gt;-config&lt;/string&gt;\n        &lt;string&gt;/opt/config.yml&lt;/string&gt;\n    &lt;/array&gt;\n&lt;/dict&gt;\n&lt;/plist&gt;\n</code></pre> <p>Then do</p> <pre><code>sudo chown root:wheel /Library/LaunchAgents/nebula.startup.plist\nsudo launchctl load /Library/LaunchAgents/nebula.startup.plist\n</code></pre>"},{"location":"others/servers/","title":"Servers","text":""},{"location":"others/servers/#servers","title":"Servers","text":""},{"location":"others/servers/#tinker-bell","title":"Tinker (Bell)","text":""},{"location":"others/servers/#aida","title":"AIDA","text":""},{"location":"others/template/","title":"Template","text":"<p>This is the template I base most of my docker compose on. It is valid for most of my sevices explaine below.</p>"},{"location":"others/template/#docker-compose-template","title":"Docker Compose Template","text":"<pre><code>version: '3.3'\nservices:\n  NAME:\n    image: IMAGE\n    container_name: NAME\n    restart: unless-stopped\n    volumes:\n      - /configs/NAME:/config\n      - /HDD1/foo:/foo\n      - /HDD1/bar:/bar\n\nnetworks:\n  default:\n    external:\n      name: external\n</code></pre> <p>Let's go block by block:</p>"},{"location":"others/template/#version","title":"Version","text":"<pre><code>version: '3.3'\n</code></pre> <p>I'm using 3.3 as it's widely supported and I might sometimes need some v3 features</p>"},{"location":"others/template/#name","title":"Name","text":"<pre><code>  NAME:\n    image: IMAGE\n    container_name: NAME\n</code></pre> <p>Those lines shouldn't be surprinsing to anyone, I just prefer using the same name for the container and the <code>container_name</code>. I don't usually have multiple instances of the same container so that's not a problem. The only exception is for databases where I often removes the <code>container_name</code> line so I know what service the database is related to.</p> <p>Whenever possible I use linuxserver.io's images, they follow the same kind of standars as I do so it makes everything really easy. And being a community project, I also find them very thrustworthy.</p>"},{"location":"others/template/#restart","title":"Restart","text":"<pre><code>    restart: unless-stopped\n</code></pre> <p>This line is just magic, it makes the container behave exactly as I want it to. I use <code>always</code> only for a few very important containers that should never be down.</p>"},{"location":"others/template/#volumes","title":"Volumes","text":"<pre><code>    volumes:\n      - /configs/NAME:/config\n      - /HDD1/foo:/foo\n      - /HDD1/bar:/bar\n</code></pre> <p>All my configs are in <code>/configs</code>, for the moment it is still in my root partition but I plan to make it a ZFS dataset someday for easy backup and snapshots. For my data I often follow this simple convention.</p>"},{"location":"others/template/#ports","title":"Ports","text":"<p>I try to avoid using <code>ports</code> and do most of the routing internally with dockers's hostnames (as shown in caddy)</p>"},{"location":"others/template/#network","title":"Network","text":"<pre><code>networks:\n  default:\n    external:\n      name: external\n</code></pre> <p>This part is only needed if the service need to be accessed from caddy. It makes sur that this network and Caddy are in the same network and can access each others</p> <pre><code>networks:\n  external:\n    name: external\n  internal:\n    internal: true\n  vlan200:\n    driver: macvlan\n    driver_opts:\n      parent: bond0.200\n    ipam:\n      config:\n        - subnet: \"192.168.200.0/24\"\n          ip_range: \"192.168.200.64/26\"\n          gateway: \"192.168.200.1\"\n</code></pre> <p>Sometimes I need specific network for some services, for exemple when I need to connect a database and a service I prefer to add them both to an <code>internal</code> network with only the two of them.</p> <p>Also some services need to access internet through a specific vlan and this can be done with the <code>vlan200</code> settings.</p>"},{"location":"others/zfs/","title":"ZFS","text":""},{"location":"others/zfs/#what-is-zfs","title":"What is ZFS ?","text":"<p>TODO</p>"},{"location":"others/zfs/#sanoid","title":"Sanoid","text":"<p>Sanoid is a tool to autmate snapshot on ZFS. I also use syncoid to upload some of my snapshots to <code>extra</code> as a quick backup.</p> <p>Sanoid is the only service not (yet) running from a docker container. I just install it on my main system.</p>"},{"location":"others/zfs/#install","title":"Install","text":"<pre><code>apt install git debhelper dpkg-dev build-essential\ncd /tmp\ngit clone https://github.com/jimsalterjrs/sanoid.git\ncd sanoid\ngit checkout $(git tag | grep \"^v\" | tail -n 1)\nln -s packages/debian .\ndpkg-buildpackage -uc -us\napt install ../sanoid_*_all.deb\n</code></pre>"},{"location":"others/zfs/#config","title":"Config","text":"<p>TODO update this</p> <pre><code>[HDD1/Documents]\n    use_template = production\n        recursive = yes\n[rpool/configs]\n    use_template = production\n        recursive = yes\n[HDD1/Backups]\n    use_template = production\n        recursive = yes\n[HDD1/Media]\n        use_template = media\n        recursive = yes\n[extra/Documents]\n        use_template = backup\n        recursive = yes\n[extra/Backups]\n        use_template = backup\n        recursive = yes\n[extra/configs]\n        use_template = backup\n        recursive = yes\n\n#############################\n# templates below this line #\n#############################\n\n[template_production]\n    frequently = 4\n    hourly = 24\n    daily = 7\n    monthly = 1\n    yearly = 0\n    autosnap = yes\n    autoprune = yes\n\n[template_media]\n    frequently = 0\n    hourly = 24\n    daily = 7\n    monthly = 0\n    yearly = 0\n    autosnap = yes\n    autoprune = yes\n\n[template_backup]\n    autoprune = yes\n    frequently = 0\n    hourly = 0\n    daily = 90\n    monthly = 0\n    yearly = 0\n\n    ### don't take new snapshots - snapshots on backup\n    ### datasets are replicated in from source, not\n    ### generated locally\n    autosnap = no\n\n    ### monitor hourlies and dailies, but don't warn or\n    ### crit until they're over 48h old, since replication\n    ### is typically daily only\n    hourly_warn = 2880\n    hourly_crit = 3600\n    daily_warn = 48\n    daily_crit = 60\n\n[template_ignore]\n    autosnap = no\n    autoprune = no\n    monitor = no\n</code></pre> <p>I don't want to keep old snapshot of media content, it's only there as a security. That's why I remove snapshots older than a week.</p> <p>As for the other content I keep snapshots up to a month for the moment.</p> <p>To enable sanoid I simply run:</p> <pre><code>systemctl enable sanoid.timer\nsystemctl start sanoid.timer\n</code></pre>"},{"location":"others/zfs/#syncoid","title":"Syncoid","text":"<p>Syncoid is a tool that comes with sanoid to automate zfs <code>send</code> and <code>recv</code>. I only backup important data and my configs. I don't feel the need to backup my movies because it would simply cost too much.</p> <pre><code>curl https://hc-ping.com/ID/start\nstatus=0\n/usr/sbin/syncoid -r --quiet --no-sync-snap rpool/configs extra/configs || status=1\n/usr/sbin/syncoid -r --quiet --no-sync-snap HDD1/Documents extra/Documents || status=1\n/usr/sbin/syncoid -r --quiet --no-sync-snap HDD1/Backups extra/Backups  || status=1\n\nif [[ ${status} -eq 1 ]]\nthen\n    curl https://hc-ping.com/ID/fail\nelse\n    curl https://hc-ping.com/ID\nfi\n</code></pre> <p>I run the replication every day at 3AM with this script.</p> <p>Using healthcheck.io I am alerted if the replication did not happened or went wrong.</p> <p></p> <pre><code>0 3 * * * /root/replication\n</code></pre>"},{"location":"others/services/dns/","title":"DynDNS","text":"<p>TODO</p> <pre><code>version: \"3.3\"\nservices:\n  ddns:\n    image: oznu/cloudflare-ddns\n    container_name: ddns\n    environment:\n      - API_KEY=key\n      - ZONE=gneee.tech\n      - SUBDOMAIN=celty\n    restart: unless-stopped\n</code></pre>"},{"location":"others/services/portaineragent/","title":"Portainer Agent","text":"<p>TODO to fix</p> <pre><code>version: '3.3'\nservices:\n  portainer:\n    image: portainer/agent\n    container_name: portainer_agent\n    restart: always\n    volumes:\n      - /var/lib/docker/volumes:/var/lib/docker/volumes\n      - /var/run/docker.sock:/var/run/docker.sock\n      - /:/host\n      - /configs/portainer-agent:/data\n    environment:\n      - EDGE=1\n      - EDGE_ID=fb885ca7-24cb-4c95-9db5-2aab2d101d52\n      - EDGE_KEY=aHR0cHM6Ly9wb3J0YWluZXIuZ25lZWUudGVjaHxwb3J0YWluZXIuZ25lZWUudGVjaDo4MDAwfDNmOjU2OmNmOjg4OjAxOjI0OmVkOmY1OmYwOmZlOjJjOjliOjU5OjJiOmUzOmQ1fDU\n      - CAP_HOST_MANAGEMENT=1\n</code></pre>"},{"location":"others/services/watchtower/","title":"Watchtower","text":"<p>TODO</p>"},{"location":"services/NextDNS/","title":"NextDNS","text":""},{"location":"services/Tailscale%20DNS/","title":"Tailscale DNS","text":"<pre><code>{\n    \"tagOwners\": {\n        \"tag:admin\":  [\"xorob0@github\"],\n        \"tag:server\": [\"xorob0@github\"],\n    },\n    \"acls\": [\n        {\"action\": \"accept\", \"users\": [\"*\"], \"ports\": [\"*:*\"]},\n    ],\n    \"ssh\": [\n        {\n            \"action\": \"accept\",\n            \"src\":    [\"xorob0@github\"],\n            \"dst\":    [\"tag:server\"],\n            \"users\":  [\"root\", \"autogroup:nonroot\"],\n        },\n        {\n            \"action\": \"accept\",\n            \"src\":    [\"tag:server\"],\n            \"dst\":    [\"tag:server\"],\n            \"users\":  [\"root\", \"autogroup:nonroot\"],\n        },\n    ],\n}\n</code></pre>"}]}