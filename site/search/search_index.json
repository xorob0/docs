{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u2693\ufe0e This is my documentations for my home server. You will find documentations about all kind of stuff I selfhost, from my Plex media server to this very website.","title":"Home"},{"location":"#home","text":"This is my documentations for my home server. You will find documentations about all kind of stuff I selfhost, from my Plex media server to this very website.","title":"Home"},{"location":"hardware/","text":"Hardware \u2693\ufe0e Story \u2693\ufe0e Let's start with the story of how I started sefhosting. This was on a Raspberry Pi 2B with a 2Tb usb drive, the only thing it did back then was download files that I could access via SSHFS. This was really terrible, this setup could only withstand a few downloads at a time and I had to pause all downloads to access the files. Then came Celty. I got her for 50\u20ac from a acquaintance. She was only a 4th gen i5 with 8Gb of RAM and a few hard drives. She was going to stay in my bedroom so I had to make her as silent as possible. So I got an Evo 212 and a Nanoxia Deep Silence 3 and was able to sleep next to the computer easily. At that point I installed FreeNAS on her and started playing with jails, I selfhosted a bunch of stuff, and started de-googlizing my life. Current config \u2693\ufe0e When I started working I gave Celty some upgrades and she became what she is today: CPU: i3-10100 Motherboard: MSI Z490-A PRO RAM: 1x16Go Cooler: Evo 212 Case: Nanoxia Deep Silence 3 So why did I go with this setup ? I wanted to be able to transcode 4k so I don't have to keep 2 versions of every movies, and I wanted this upgrade to be my engame for a long time and at the same time, to still be upgradable if I need to, hence the single stick of ram and the Z490 chipset (that will be compatible with 11th gen intel core and their Xe iGPUs). This motherboard also has 2.5Gb ethernet which may be important for the future. Hard Drives \u2693\ufe0e I currently have 6 hard drives in this box: 2*4To 2*6To 1*12To 1*480Go SSD Everything is SATA for the moment. I hit a point now where I have as much drives as SATA ports, that means two important things: If I need all my drives at the same time, I don't have a port available to replace a disk if it starts behaving For my next upgrade I'll need a PCIe card which is either expensive or shitty","title":"Hardware"},{"location":"hardware/#hardware","text":"","title":"Hardware"},{"location":"hardware/#story","text":"Let's start with the story of how I started sefhosting. This was on a Raspberry Pi 2B with a 2Tb usb drive, the only thing it did back then was download files that I could access via SSHFS. This was really terrible, this setup could only withstand a few downloads at a time and I had to pause all downloads to access the files. Then came Celty. I got her for 50\u20ac from a acquaintance. She was only a 4th gen i5 with 8Gb of RAM and a few hard drives. She was going to stay in my bedroom so I had to make her as silent as possible. So I got an Evo 212 and a Nanoxia Deep Silence 3 and was able to sleep next to the computer easily. At that point I installed FreeNAS on her and started playing with jails, I selfhosted a bunch of stuff, and started de-googlizing my life.","title":"Story"},{"location":"hardware/#current-config","text":"When I started working I gave Celty some upgrades and she became what she is today: CPU: i3-10100 Motherboard: MSI Z490-A PRO RAM: 1x16Go Cooler: Evo 212 Case: Nanoxia Deep Silence 3 So why did I go with this setup ? I wanted to be able to transcode 4k so I don't have to keep 2 versions of every movies, and I wanted this upgrade to be my engame for a long time and at the same time, to still be upgradable if I need to, hence the single stick of ram and the Z490 chipset (that will be compatible with 11th gen intel core and their Xe iGPUs). This motherboard also has 2.5Gb ethernet which may be important for the future.","title":"Current config"},{"location":"hardware/#hard-drives","text":"I currently have 6 hard drives in this box: 2*4To 2*6To 1*12To 1*480Go SSD Everything is SATA for the moment. I hit a point now where I have as much drives as SATA ports, that means two important things: If I need all my drives at the same time, I don't have a port available to replace a disk if it starts behaving For my next upgrade I'll need a PCIe card which is either expensive or shitty","title":"Hard Drives"},{"location":"license/","text":"","title":"License"},{"location":"software/","text":"Software \u2693\ufe0e Story \u2693\ufe0e The Software side of things had a lot more iterations than the hardware side. Back on my Raspberry Pi I was on Open Media Vault and really didn't like it. Every update broke the system, but I didn't have a lot of experience back then so that may be my fault too \ud83e\udd37. On Celty I used FreeNAS for a long time. Back in the days it was basically the only way to have ZFS with good performances. I kept FreeNAS for a few years until the release of OpenZFS 2.0.0 which made it possible to move to Linux. At that time I a friend was setting up his server with OMV and docker and the simply of docker really intrigued me. That's why on December 2020 I moved to Ubuntu 20.04 with docker and OpenZFS. I loved it even if the OpenZFS setup was quite messy because it was not yet officially supported by Ubuntu. Current config \u2693\ufe0e Operating System \u2693\ufe0e A few weeks later I upgraded my hardware and kind of broke my install. I decided to go with Proxmox as it was supposed to have a really up to date OpenZFS version. I discovered this was false and had to install the beta kernel to have it working. In hindsight, Proxmox is really not the best OS for me, but I'll stick to it until I get my hand on 2 NVME drive and need to re-install everything. I'll probably go with an Ubuntu install again but this time with ZFS for my SSD to use snapshots on my configs. ZFS \u2693\ufe0e With my latest disk upgrade I started managing 2 pools, a stripped pool with most of my disks (badly named HDD1 ) and an other stripped pool with the disks I don't want in my main pool (named extra ). As explained above, I don't want my main pool to ever gets bigger than 4 disks, this reduces the risk that one of my disk would fail and also let me keep one SATA port available at all time in case I need to replace a drive. That maximun number of disk might increase to 5 when I get some NVME drives for my OS. TODO: datasets \u2693\ufe0e https://github.com/fabianishere/pve-edge-kernel https://pve.proxmox.com/wiki/Package_Repositories https://github.com/jimsalterjrs/sanoid/blob/master/INSTALL.md rsync /configs rsync /var/lib/docker https://raw.githubusercontent.com/portainer/templates/master/templates-2.0.json","title":"Software"},{"location":"software/#software","text":"","title":"Software"},{"location":"software/#story","text":"The Software side of things had a lot more iterations than the hardware side. Back on my Raspberry Pi I was on Open Media Vault and really didn't like it. Every update broke the system, but I didn't have a lot of experience back then so that may be my fault too \ud83e\udd37. On Celty I used FreeNAS for a long time. Back in the days it was basically the only way to have ZFS with good performances. I kept FreeNAS for a few years until the release of OpenZFS 2.0.0 which made it possible to move to Linux. At that time I a friend was setting up his server with OMV and docker and the simply of docker really intrigued me. That's why on December 2020 I moved to Ubuntu 20.04 with docker and OpenZFS. I loved it even if the OpenZFS setup was quite messy because it was not yet officially supported by Ubuntu.","title":"Story"},{"location":"software/#current-config","text":"","title":"Current config"},{"location":"software/#operating-system","text":"A few weeks later I upgraded my hardware and kind of broke my install. I decided to go with Proxmox as it was supposed to have a really up to date OpenZFS version. I discovered this was false and had to install the beta kernel to have it working. In hindsight, Proxmox is really not the best OS for me, but I'll stick to it until I get my hand on 2 NVME drive and need to re-install everything. I'll probably go with an Ubuntu install again but this time with ZFS for my SSD to use snapshots on my configs.","title":"Operating System"},{"location":"software/#zfs","text":"With my latest disk upgrade I started managing 2 pools, a stripped pool with most of my disks (badly named HDD1 ) and an other stripped pool with the disks I don't want in my main pool (named extra ). As explained above, I don't want my main pool to ever gets bigger than 4 disks, this reduces the risk that one of my disk would fail and also let me keep one SATA port available at all time in case I need to replace a drive. That maximun number of disk might increase to 5 when I get some NVME drives for my OS.","title":"ZFS"},{"location":"software/#todo-datasets","text":"https://github.com/fabianishere/pve-edge-kernel https://pve.proxmox.com/wiki/Package_Repositories https://github.com/jimsalterjrs/sanoid/blob/master/INSTALL.md rsync /configs rsync /var/lib/docker https://raw.githubusercontent.com/portainer/templates/master/templates-2.0.json","title":"TODO: datasets"},{"location":"services/librephotos/","text":"LibrePhotos \u2693\ufe0e LibrePhotos is a really powerful web gallery application. I use it alongside NextCloud as a google photo alternative. The upload and management of the pictures is handled by NextCloud , while LibrePhoto takes care of organizing it, matching faces and creating albums using A.I. Compose \u2693\ufe0e version: '3.3' services: proxy: image: reallibrephotos/librephotos-proxy:dev restart: always volumes: - /HDD1/Documents/nextcloud/tim/files/Photos/:/data - /configs/librephotos/proxy/protected_media:/protected_media depends_on: - backend - frontend db: image: postgres restart: always environment: - POSTGRES_USER=docker - POSTGRES_PASSWORD=password - POSTGRES_DB=librephotos volumes: - /configs/librephotos/db:/var/lib/postgresql/data command: postgres -c fsync=off -c synchronous_commit=off -c full_page_writes=off -c random_page_cost=1.0 frontend: image: reallibrephotos/librephotos-frontend:dev restart: always depends_on: - backend backend: image: reallibrephotos/librephotos:dev restart: always volumes: - /HDD1/Documents/nextcloud/tim/files/Photos/:/data - /configs/librephotos/proxy/protected_media:/protected_media - /configs/librephotos/logs:/logs - /configs/librephotos/cache:/root/.cache environment: - SECRET_KEY=secret - BACKEND_HOST=backend - ADMIN_EMAIL=admin@example.com - ADMIN_USERNAME=admin - ADMIN_PASSWORD=admin - DB_BACKEND=postgresql - DB_NAME=librephotos - DB_USER=docker - DB_PASS=password - DB_HOST=db - DB_PORT=5432 - REDIS_HOST=redis - REDIS_PORT=6379 - MAPBOX_API_KEY=${KEY} - WEB_CONCURRENCY=4 - TIME_ZONE=Europe/Brussels - SKIP_PATTERNS= - DEBUG=0 # Wait for Postgres depends_on: - db redis: image: redis restart: always This one is a bit more tricky as it require a backend, a frontend, a database and a proxy. Of course if you're going to copy-paste this compose you should change the user names, the password and the keys.","title":"LibrePhotos"},{"location":"services/librephotos/#librephotos","text":"LibrePhotos is a really powerful web gallery application. I use it alongside NextCloud as a google photo alternative. The upload and management of the pictures is handled by NextCloud , while LibrePhoto takes care of organizing it, matching faces and creating albums using A.I.","title":"LibrePhotos"},{"location":"services/librephotos/#compose","text":"version: '3.3' services: proxy: image: reallibrephotos/librephotos-proxy:dev restart: always volumes: - /HDD1/Documents/nextcloud/tim/files/Photos/:/data - /configs/librephotos/proxy/protected_media:/protected_media depends_on: - backend - frontend db: image: postgres restart: always environment: - POSTGRES_USER=docker - POSTGRES_PASSWORD=password - POSTGRES_DB=librephotos volumes: - /configs/librephotos/db:/var/lib/postgresql/data command: postgres -c fsync=off -c synchronous_commit=off -c full_page_writes=off -c random_page_cost=1.0 frontend: image: reallibrephotos/librephotos-frontend:dev restart: always depends_on: - backend backend: image: reallibrephotos/librephotos:dev restart: always volumes: - /HDD1/Documents/nextcloud/tim/files/Photos/:/data - /configs/librephotos/proxy/protected_media:/protected_media - /configs/librephotos/logs:/logs - /configs/librephotos/cache:/root/.cache environment: - SECRET_KEY=secret - BACKEND_HOST=backend - ADMIN_EMAIL=admin@example.com - ADMIN_USERNAME=admin - ADMIN_PASSWORD=admin - DB_BACKEND=postgresql - DB_NAME=librephotos - DB_USER=docker - DB_PASS=password - DB_HOST=db - DB_PORT=5432 - REDIS_HOST=redis - REDIS_PORT=6379 - MAPBOX_API_KEY=${KEY} - WEB_CONCURRENCY=4 - TIME_ZONE=Europe/Brussels - SKIP_PATTERNS= - DEBUG=0 # Wait for Postgres depends_on: - db redis: image: redis restart: always This one is a bit more tricky as it require a backend, a frontend, a database and a proxy. Of course if you're going to copy-paste this compose you should change the user names, the password and the keys.","title":"Compose"},{"location":"services/plex/","text":"Plex \u2693\ufe0e Compose \u2693\ufe0e version: '3.3' services: plex: image: ghcr.io/linuxserver/plex container_name: plex restart: unless-stopped environment: - VERSION=docker volumes: - /configs/plex:/config - /HDD1/Media/TV-Shows:/tv - /HDD1/Media/Movies:/movies - /HDD1/Media/Lives:/lives - /HDD1/Media/courses:/courses - /HDD1/Media/Workout:/wourkouts tmpfs: - /tmp devices: - /dev/dri:/dev/dri tmpfs \u2693\ufe0e Nothing really special on this docker-compose, except for the tmpfs line. You can thing of tmpfs as a \"mountpoint to RAM\". Every files written to /tmp is written to RAM. I am using this so my transcoded files don't have to be written to my SSD (because it would worn it out pretty fast) or my ZFS pool (which is slow in comparaison to RAM or my SSD). devices \u2693\ufe0e To use QuickSync for transcoding I need to pass /dev/dri to the container. After that everything just works. Tips \u2693\ufe0e Put your thumnails on your SSD \u2693\ufe0e I used to have my thumbnails and my preview images on my HDD, this makes the UI feels really slow. But on the other hand these images can take up a lot of space, I'm currently at 50Go for my Plex Media Server folder. This could be mitigated with a CDN but might get you bad performances on local network. Get Plex Pass \u2693\ufe0e If you want to use Plex the way I am, get a Plex Pass! This will unlock some really nice features and also support the devs Disable Online Media Sources \u2693\ufe0e One of the first things I did was disabling the free plex sources, I don't use them and find that they mostly clutter the UI. The only one I kept enabled is the Tidal one. I advise you to keep it enabled and add a free tidal account. This will add the soudtracks to your movies and TV shows. Remote access \u2693\ufe0e Plex comes with integrated remote access management. It works well, but does not allow for a lot of optimisation. I decided to not use it. Instead I set up a \"Custom server access URL\" under \"Network\" For this to work I am routing with Caddy #TODO: add link to caddy This has several benefits: - I don't get blocked by firewall when trying to listen to my music at the ofice - I can use cloudflare's CDN for my images (they don't allow videos to be cached anymore) - Every services available outside my network is managed by my reverse proxy, meaning I have more control about it. You need to specify the port even if using some standard http/https ports. Don't use a CDN if you're going to use Plex locally \u2693\ufe0e If you use a CDN evey stream will have to go through the CDN, this will give you pretty bad perfomances on local network even if it can improves performances remotely. This could be circumvented with some DNS magic but I did not try it yet. Transcoder settings \u2693\ufe0e If you want to use tmpfs as your transcoding folder, you should set it in the \"Transcoder\" settings: I advise you to use /tmp since I had some issues using other directories. It seems like the library used for EAC decoding was not using the path set up in the settings and would always go to /tmp . If it is not enabled, you should enable: - HDR tonemapping: This makes HDR content compatible with SDR devices. If disabled your HDR content will look washed out on SDR devices - Use hardware acceleration when available: This will use your GPU instead of you CPU for transocding Enable agents \u2693\ufe0e In the \"Agents\" settings you can add some agents that require API keys, they will make more content available like alternatives posters for your movies. They might also help with content matching.","title":"Plex"},{"location":"services/plex/#plex","text":"","title":"Plex"},{"location":"services/plex/#compose","text":"version: '3.3' services: plex: image: ghcr.io/linuxserver/plex container_name: plex restart: unless-stopped environment: - VERSION=docker volumes: - /configs/plex:/config - /HDD1/Media/TV-Shows:/tv - /HDD1/Media/Movies:/movies - /HDD1/Media/Lives:/lives - /HDD1/Media/courses:/courses - /HDD1/Media/Workout:/wourkouts tmpfs: - /tmp devices: - /dev/dri:/dev/dri","title":"Compose"},{"location":"services/plex/#tmpfs","text":"Nothing really special on this docker-compose, except for the tmpfs line. You can thing of tmpfs as a \"mountpoint to RAM\". Every files written to /tmp is written to RAM. I am using this so my transcoded files don't have to be written to my SSD (because it would worn it out pretty fast) or my ZFS pool (which is slow in comparaison to RAM or my SSD).","title":"tmpfs"},{"location":"services/plex/#devices","text":"To use QuickSync for transcoding I need to pass /dev/dri to the container. After that everything just works.","title":"devices"},{"location":"services/plex/#tips","text":"","title":"Tips"},{"location":"services/plex/#put-your-thumnails-on-your-ssd","text":"I used to have my thumbnails and my preview images on my HDD, this makes the UI feels really slow. But on the other hand these images can take up a lot of space, I'm currently at 50Go for my Plex Media Server folder. This could be mitigated with a CDN but might get you bad performances on local network.","title":"Put your thumnails on your SSD"},{"location":"services/plex/#get-plex-pass","text":"If you want to use Plex the way I am, get a Plex Pass! This will unlock some really nice features and also support the devs","title":"Get Plex Pass"},{"location":"services/plex/#disable-online-media-sources","text":"One of the first things I did was disabling the free plex sources, I don't use them and find that they mostly clutter the UI. The only one I kept enabled is the Tidal one. I advise you to keep it enabled and add a free tidal account. This will add the soudtracks to your movies and TV shows.","title":"Disable Online Media Sources"},{"location":"services/plex/#remote-access","text":"Plex comes with integrated remote access management. It works well, but does not allow for a lot of optimisation. I decided to not use it. Instead I set up a \"Custom server access URL\" under \"Network\" For this to work I am routing with Caddy #TODO: add link to caddy This has several benefits: - I don't get blocked by firewall when trying to listen to my music at the ofice - I can use cloudflare's CDN for my images (they don't allow videos to be cached anymore) - Every services available outside my network is managed by my reverse proxy, meaning I have more control about it. You need to specify the port even if using some standard http/https ports.","title":"Remote access"},{"location":"services/plex/#dont-use-a-cdn-if-youre-going-to-use-plex-locally","text":"If you use a CDN evey stream will have to go through the CDN, this will give you pretty bad perfomances on local network even if it can improves performances remotely. This could be circumvented with some DNS magic but I did not try it yet.","title":"Don't use a CDN if you're going to use Plex locally"},{"location":"services/plex/#transcoder-settings","text":"If you want to use tmpfs as your transcoding folder, you should set it in the \"Transcoder\" settings: I advise you to use /tmp since I had some issues using other directories. It seems like the library used for EAC decoding was not using the path set up in the settings and would always go to /tmp . If it is not enabled, you should enable: - HDR tonemapping: This makes HDR content compatible with SDR devices. If disabled your HDR content will look washed out on SDR devices - Use hardware acceleration when available: This will use your GPU instead of you CPU for transocding","title":"Transcoder settings"},{"location":"services/plex/#enable-agents","text":"In the \"Agents\" settings you can add some agents that require API keys, they will make more content available like alternatives posters for your movies. They might also help with content matching.","title":"Enable agents"},{"location":"services/radarr/","text":"Radarr \u2693\ufe0e","title":"Radarr"},{"location":"services/radarr/#radarr","text":"","title":"Radarr"},{"location":"services/sanoid/","text":"Sanoid \u2693\ufe0e Sanoid is a tool to autmate snapshot on ZFS. I also use syncoid to upload some of my snapshots to extra as a quick backup. Config \u2693\ufe0e [HDD1/Documents] frequently = 0 hourly = 36 daily = 30 monthly = 3 yearly = 1 autosnap = yes autoprune = yes [HDD1/Backups] frequently = 0 hourly = 36 daily = 30 monthly = 3 yearly = 1 autosnap = yes autoprune = yes [HDD1/Media] frequently = 0 hourly = 36 daily = 30 monthly = 2 yearly = 0 autosnap = yes autoprune = yes [extra/Documents] autoprune = yes frequently = 0 hourly = 0 daily = 90 monthly = 12 yearly = 0 autosnap = no hourly_warn = 2880 hourly_crit = 3600 daily_warn = 48 daily_crit = 60 [extra/Backups] autoprune = yes frequently = 0 hourly = 0 daily = 90 monthly = 12 yearly = 0 autosnap = no hourly_warn = 2880 hourly_crit = 3600 daily_warn = 48 daily_crit = 60 As for the cron jobs, I am using: Cron \u2693\ufe0e * * * * * TZ=UTC /usr/sbin/sanoid --cron 0 3 * * * syncoid HDD1/Documents extra/Documents 0 3 * * * syncoid HDD1/Backups extra/Backups This will take the automatic snapshots and move some to extra every days at 3AM","title":"Sanoid"},{"location":"services/sanoid/#sanoid","text":"Sanoid is a tool to autmate snapshot on ZFS. I also use syncoid to upload some of my snapshots to extra as a quick backup.","title":"Sanoid"},{"location":"services/sanoid/#config","text":"[HDD1/Documents] frequently = 0 hourly = 36 daily = 30 monthly = 3 yearly = 1 autosnap = yes autoprune = yes [HDD1/Backups] frequently = 0 hourly = 36 daily = 30 monthly = 3 yearly = 1 autosnap = yes autoprune = yes [HDD1/Media] frequently = 0 hourly = 36 daily = 30 monthly = 2 yearly = 0 autosnap = yes autoprune = yes [extra/Documents] autoprune = yes frequently = 0 hourly = 0 daily = 90 monthly = 12 yearly = 0 autosnap = no hourly_warn = 2880 hourly_crit = 3600 daily_warn = 48 daily_crit = 60 [extra/Backups] autoprune = yes frequently = 0 hourly = 0 daily = 90 monthly = 12 yearly = 0 autosnap = no hourly_warn = 2880 hourly_crit = 3600 daily_warn = 48 daily_crit = 60 As for the cron jobs, I am using:","title":"Config"},{"location":"services/sanoid/#cron","text":"* * * * * TZ=UTC /usr/sbin/sanoid --cron 0 3 * * * syncoid HDD1/Documents extra/Documents 0 3 * * * syncoid HDD1/Backups extra/Backups This will take the automatic snapshots and move some to extra every days at 3AM","title":"Cron"},{"location":"services/template/","text":"Template \u2693\ufe0e This is the template I base most of my docker compose on. It is valid for most of my sevices explaine below. Docker Compose Template \u2693\ufe0e version: '3.3' services: ${NAME}: image: ${IMAGE} container_name: ${NAME} restart: unless-stopped volumes: - /configs/${NAME}:/etc/caddy/Caddyfile:ro # to mount custom Caddyfile - /HDD1/foo:/foo - /HDD1/bar:/bar Let's go block by block: Version \u2693\ufe0e version: '3.3' I'm using 3.3 as it's widely supported and I might sometimes need some v3 features Name \u2693\ufe0e ${NAME}: image: ${IMAGE} container_name: ${NAME} Those lines shouldn't be surprinsing to anyone, I just prefer using the same name for the container and the container_name . I don't usually have multiple instances of the same container so that's not a problem. The only exception is for databases where I often removes the container_name line so I know what service the database is related to. Whenever possible I use linuxserver.io 's images, they follow the same kind of standars as I do so it makes everything really easy. And being a community project, I also find them very thrustworthy. Restart \u2693\ufe0e restart: unless-stopped This line is just magic, it makes the container behave exactly as I want it to. I use always only for a few very important containers that should never be down. Volumes \u2693\ufe0e volumes: - /configs/${NAME}:/etc/caddy/Caddyfile:ro # to mount custom Caddyfile - /HDD1/foo:/foo - /HDD1/bar:/bar All my configs are in /configs , for the moment it is still in my root partition but I plan to make it a ZFS dataset someday for easy backup and snapshots. For my data I often follow this simple convention. Ports \u2693\ufe0e I try to avoid using ports and do most of the routing internally with dockers's hostnames (as shown in caddy ) TODO: Add link to caddy","title":"Template"},{"location":"services/template/#template","text":"This is the template I base most of my docker compose on. It is valid for most of my sevices explaine below.","title":"Template"},{"location":"services/template/#docker-compose-template","text":"version: '3.3' services: ${NAME}: image: ${IMAGE} container_name: ${NAME} restart: unless-stopped volumes: - /configs/${NAME}:/etc/caddy/Caddyfile:ro # to mount custom Caddyfile - /HDD1/foo:/foo - /HDD1/bar:/bar Let's go block by block:","title":"Docker Compose Template"},{"location":"services/template/#version","text":"version: '3.3' I'm using 3.3 as it's widely supported and I might sometimes need some v3 features","title":"Version"},{"location":"services/template/#name","text":"${NAME}: image: ${IMAGE} container_name: ${NAME} Those lines shouldn't be surprinsing to anyone, I just prefer using the same name for the container and the container_name . I don't usually have multiple instances of the same container so that's not a problem. The only exception is for databases where I often removes the container_name line so I know what service the database is related to. Whenever possible I use linuxserver.io 's images, they follow the same kind of standars as I do so it makes everything really easy. And being a community project, I also find them very thrustworthy.","title":"Name"},{"location":"services/template/#restart","text":"restart: unless-stopped This line is just magic, it makes the container behave exactly as I want it to. I use always only for a few very important containers that should never be down.","title":"Restart"},{"location":"services/template/#volumes","text":"volumes: - /configs/${NAME}:/etc/caddy/Caddyfile:ro # to mount custom Caddyfile - /HDD1/foo:/foo - /HDD1/bar:/bar All my configs are in /configs , for the moment it is still in my root partition but I plan to make it a ZFS dataset someday for easy backup and snapshots. For my data I often follow this simple convention.","title":"Volumes"},{"location":"services/template/#ports","text":"I try to avoid using ports and do most of the routing internally with dockers's hostnames (as shown in caddy ) TODO: Add link to caddy","title":"Ports"},{"location":"services/wireguard/","text":"WireGuard \u2693\ufe0e Wireguard is a performant vpn with client on almost all platforms. Compose \u2693\ufe0e --- version: \"3.3\" services: wireguard: image: ghcr.io/linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - SERVERURL=wireguard.gneee.tech - PEERS=laptop,phone volumes: - /configs/wireguard:/config - /lib/modules:/lib/modules ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped Nothing to add here, this is basicaly pasted from linuxserver.io . Keep in mind that you have to add the port forwarding in you router. NOTE: As this is udp on port 51820 this can be blocked by some the firewall on some company networks. Usage \u2693\ufe0e To show the configs for your phone, use: docker exec -it wireguard /app/show-peer phone and scan the QR code If you need to access the config file you can find it at /configs/wireguard/peer_laptop/peer_laptop.conf or you can upload it to linx by typing: curl -T /configs/wireguard/peer_laptop/peer_laptop.conf https://linx.gneee.tech/upload","title":"WireGuard"},{"location":"services/wireguard/#wireguard","text":"Wireguard is a performant vpn with client on almost all platforms.","title":"WireGuard"},{"location":"services/wireguard/#compose","text":"--- version: \"3.3\" services: wireguard: image: ghcr.io/linuxserver/wireguard container_name: wireguard cap_add: - NET_ADMIN - SYS_MODULE environment: - SERVERURL=wireguard.gneee.tech - PEERS=laptop,phone volumes: - /configs/wireguard:/config - /lib/modules:/lib/modules ports: - 51820:51820/udp sysctls: - net.ipv4.conf.all.src_valid_mark=1 restart: unless-stopped Nothing to add here, this is basicaly pasted from linuxserver.io . Keep in mind that you have to add the port forwarding in you router. NOTE: As this is udp on port 51820 this can be blocked by some the firewall on some company networks.","title":"Compose"},{"location":"services/wireguard/#usage","text":"To show the configs for your phone, use: docker exec -it wireguard /app/show-peer phone and scan the QR code If you need to access the config file you can find it at /configs/wireguard/peer_laptop/peer_laptop.conf or you can upload it to linx by typing: curl -T /configs/wireguard/peer_laptop/peer_laptop.conf https://linx.gneee.tech/upload","title":"Usage"}]}